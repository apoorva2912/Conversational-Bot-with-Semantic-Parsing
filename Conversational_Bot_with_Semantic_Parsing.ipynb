{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Team Design Challenge: Conversational Bot with Semantic Parsing\n",
        "\n",
        "**Purpose Statement:** The purpose of this project is to create a functional chatbot that is trained to a customer database which parses text input and returns corresponding SQL query as an output.\n",
        "\n",
        "We utilized the Chat GPT-3.5-turbo api in order to generate robust text-to-SQL queries.\n",
        "\n",
        "To measure accuracy, we used the Levenshtein distance to determine how close our model's predictions were to the real output.\n",
        "\n",
        "All processing time was recorded and can be clearly shown while running individual code blocks.\n",
        "\n",
        "\n",
        "Team Members\n",
        "1. Apoorva Rastogi\n",
        "2. Nandita Narayanan\n",
        "3. Jibi Jacob\n",
        "4. Shanmukh Sanka"
      ],
      "metadata": {
        "id": "aPYUFksB4JbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        ">[Team Design Challenge: Conversational Bot with Semantic Parsing](#scrollTo=aPYUFksB4JbC)\n",
        "\n",
        ">[Setup: Library/Module Imports](#scrollTo=_jh0xWXS8FPO)\n",
        "\n",
        ">[Database Setup with sqlite3](#scrollTo=UC7U9ucV8paS)\n",
        "\n",
        ">[Mounting Google Drive](#scrollTo=S3llhaQ1-cGo)\n",
        "\n",
        ">[Querying the Initial Customer Database/Data Exploration](#scrollTo=Fc07DVoLpJO-)\n",
        "\n",
        ">[Initial Functions for Model Input/Output](#scrollTo=Fwwm4J8V9_8M)\n",
        "\n",
        ">[Final Solution](#scrollTo=onYyAqeSqzcj)\n",
        "\n",
        ">[Training, Test, and Validation Output Files](#scrollTo=mKzTZ__rrtgm)\n",
        "\n",
        ">[Training Data](#scrollTo=QXIILVegspkx)\n",
        "\n",
        ">[Test Data](#scrollTo=KrpruqWOsuTw)\n",
        "\n",
        ">[Validation Data](#scrollTo=vMTkUFhVsxUU)\n",
        "\n",
        ">[Evaluating the Accuracy of the Model: Levenshtein Distance](#scrollTo=HCvhEeGus2DP)\n",
        "\n",
        ">[Results Analysis](#scrollTo=-WtZ4oJpwXjh)\n",
        "\n",
        ">[Future Avenues of Development](#scrollTo=pBqgf9rGuJJt)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "0TQgCz8LywkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup: Library/Module Imports\n",
        "- **openpyxl:** used to read from the excel files comprising our database and write the model output to the training, test, and validation excel files.\n",
        "- **sqlite3:** a module used to integrate our created SQLite database with Python.\n",
        "- **pandas:** used for data manipulation, file input/output, and creating dataframes for the database.\n",
        "- **openai:** used the GPT-3.5 model to generate text-to-sql output. Provides convenient access to the OpenAI API from applications written in Python.\n",
        "- **gradio:** used to create a simple, easy-to-use, interactive app that allows users to try out the demo in their browsers on google colab.\n",
        "- **transformers:** used to easily download and train ML models.\n",
        "- **Levenshtein:**  a string metric for measuring the difference between two sequences. The similarity score was used as a quantitative measure for determining the accuracy of our data."
      ],
      "metadata": {
        "id": "_jh0xWXS8FPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KS6bAdaukqi",
        "outputId": "e4e69a5c-02d2-4992-c5aa-14516526e112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Database Setup with sqlite3\n",
        "\n",
        "We utlized sqlite3 to set up the database and schema which will be fed into the GPT-3.5 model alongside the queries to aid in the text-to-sql conversion. Our database creation is robust as we careate a series of dataframes which correspond to the individual tables and then synthesize them into a dictionary. Afterwards, we iterate through the dictionary and add each table to the SQL database, which is named 'my_database.db'"
      ],
      "metadata": {
        "id": "UC7U9ucV8paS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conn = sqlite3.connect('my_database.db')\n",
        "\n",
        "\n",
        "df1 = pd.read_excel('/content/drive/MyDrive/Marketing List Chatbot/Database/communication.xlsx', engine='openpyxl')\n",
        "df2 = pd.read_excel('/content/drive/MyDrive/Marketing List Chatbot/Database/customers.xlsx', engine='openpyxl')\n",
        "df3 = pd.read_excel('/content/drive/MyDrive/Marketing List Chatbot/Database/interactions.xlsx', engine='openpyxl')\n",
        "df4 = pd.read_excel('/content/drive/MyDrive/Marketing List Chatbot/Database/life_events.xlsx', engine='openpyxl')\n",
        "df5 = pd.read_excel('/content/drive/MyDrive/Marketing List Chatbot/Database/marketing_campaign.xlsx', engine='openpyxl')\n",
        "df6 = pd.read_excel('/content/drive/MyDrive/Marketing List Chatbot/Database/products.xlsx', engine='openpyxl')\n",
        "df7 = pd.read_excel('/content/drive/MyDrive/Marketing List Chatbot/Database/transactions.xlsx', engine='openpyxl')\n",
        "\n",
        "# Creating a dictionary\n",
        "dfs = {\n",
        "    \"communication\": df1,\n",
        "    \"customers\": df2,\n",
        "    \"interactions\": df3,\n",
        "    \"life_events\": df4,\n",
        "    \"marketing_campaign\": df5,\n",
        "    \"products\": df6,\n",
        "    \"transactions\": df7\n",
        "}\n",
        "\n",
        "# Loop through each dataframe and create a table for each\n",
        "for table_name, df in dfs.items():\n",
        "    # Insert the DataFrame content into a new SQL table\n",
        "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n"
      ],
      "metadata": {
        "id": "yx7E9t7tTSzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "Mounting our google drive allows the individual excel files that comprise the database to be easily accessed through the team's Google Drive account to create the database and schema in SQL. Mounting the Google Drive allows for seamless integration of files and large datasets necessary for data science or analytics."
      ],
      "metadata": {
        "id": "S3llhaQ1-cGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RjFo88f6qvQ",
        "outputId": "82807704-d365-429c-fa96-9802138fd972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Querying the Initial Customer Database/Data Exploration\n",
        "\n",
        "We tested simple and complex queries on our created database in order to verify that it was created and designed properly and that the model would be able to easily retrieve information through generated queries. We used pandas to read and interperet the queries. This step was critical in the data exploration process and to ensure a robust database."
      ],
      "metadata": {
        "id": "Fc07DVoLpJO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect('my_database.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "allDataInfo=''\n",
        "for table_name in dfs.keys():\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
        "    columns = [column[1] for column in cursor.fetchall()]\n",
        "    currentinfo = f\"Table : {table_name} Columns: {columns}\"\n",
        "    allDataInfo  = allDataInfo + currentinfo\n",
        "    print( currentinfo )\n",
        "\n",
        "\n",
        "print(allDataInfo)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ymnNuz9Ukk4",
        "outputId": "451aafb4-be3f-4782-9ace-061d2caa7994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table : communication Columns: ['row_id', 'communication_id', 'cust_id', 'contact_communication_date', 'contact_communication_channel', 'contact_communication_type', 'contact_communication_details']\n",
            "Table : customers Columns: ['row_id', 'customer_id', 'first_name', 'middle_name', 'last_name', 'gender', 'date_of_birth', 'age', 'marital_status', 'number_of_dependants', 'email_address', 'mobile_phone_number', 'work_phone_number', 'home_phone_number', 'mailing_address1', 'mailing_address2', 'city', 'state', 'zip_postal_code', 'country', 'income_level', 'credit_score', 'distance_nearest_branch']\n",
            "Table : interactions Columns: ['row_id', 'interactions_id', 'cust_id', 'interaction_date', 'interaction_channel', 'interaction_type', 'interaction_details']\n",
            "Table : life_events Columns: ['row_id', 'life_events_id', 'cust_id', 'life_event_date', 'life_event_type']\n",
            "Table : marketing_campaign Columns: ['row_id', 'marketing_campaign_id', 'cust_id', 'marketing_campaign_response', 'product_interest', 'marketing_campaign_response_channel', 'marketing_campaign_cost', 'marketing_campaign_days', 'marketing_campaign_startdate', 'marketing_campaign_enddate']\n",
            "Table : products Columns: ['row_id', 'product_id', 'cust_id', 'product_application_date', 'product', 'application_decision']\n",
            "Table : transactions Columns: ['row_id', 'transaction_id', 'cust_id', 'transaction_date', 'transaction_amount', 'transaction_details']\n",
            "Table : communication Columns: ['row_id', 'communication_id', 'cust_id', 'contact_communication_date', 'contact_communication_channel', 'contact_communication_type', 'contact_communication_details']Table : customers Columns: ['row_id', 'customer_id', 'first_name', 'middle_name', 'last_name', 'gender', 'date_of_birth', 'age', 'marital_status', 'number_of_dependants', 'email_address', 'mobile_phone_number', 'work_phone_number', 'home_phone_number', 'mailing_address1', 'mailing_address2', 'city', 'state', 'zip_postal_code', 'country', 'income_level', 'credit_score', 'distance_nearest_branch']Table : interactions Columns: ['row_id', 'interactions_id', 'cust_id', 'interaction_date', 'interaction_channel', 'interaction_type', 'interaction_details']Table : life_events Columns: ['row_id', 'life_events_id', 'cust_id', 'life_event_date', 'life_event_type']Table : marketing_campaign Columns: ['row_id', 'marketing_campaign_id', 'cust_id', 'marketing_campaign_response', 'product_interest', 'marketing_campaign_response_channel', 'marketing_campaign_cost', 'marketing_campaign_days', 'marketing_campaign_startdate', 'marketing_campaign_enddate']Table : products Columns: ['row_id', 'product_id', 'cust_id', 'product_application_date', 'product', 'application_decision']Table : transactions Columns: ['row_id', 'transaction_id', 'cust_id', 'transaction_date', 'transaction_amount', 'transaction_details']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_result = pd.read_sql('SELECT * FROM transactions LIMIT 5', conn)\n",
        "print(query_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1nXyOz2U9DO",
        "outputId": "8b1d7ab0-5d26-468e-d314-6fff4acdc139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   row_id  transaction_id          cust_id     transaction_date  \\\n",
            "0   18301  txnid_10018301  custid_10000001  2021-05-10 00:00:00   \n",
            "1   21067  txnid_10021067  custid_10000001  2022-05-31 00:00:00   \n",
            "2   73544  txnid_10073544  custid_10000001  2022-01-27 00:00:00   \n",
            "3   92977  txnid_10092977  custid_10000001  2021-05-08 00:00:00   \n",
            "4  103592  txnid_10103592  custid_10000001  2021-10-16 00:00:00   \n",
            "\n",
            "   transaction_amount           transaction_details  \n",
            "0              209.67    Grocery shopping - Walmart  \n",
            "1              879.36   Restaurant bill - Pizza Hut  \n",
            "2               43.66          Fitness class - Yoga  \n",
            "3              814.92       Car rental - Enterprise  \n",
            "4               95.36  Car maintenance - Oil change  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect('my_database.db')\n"
      ],
      "metadata": {
        "id": "zoy4_PtbNTo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_result = pd.read_sql(\"\"\"\n",
        "SELECT\n",
        "    life_events.cust_id,\n",
        "    life_events.life_event_date,\n",
        "    transactions.transaction_date,\n",
        "    interactions.interaction_date\n",
        "FROM\n",
        "    life_events\n",
        "JOIN\n",
        "    transactions ON life_events.cust_id = transactions.cust_id\n",
        "                AND life_events.life_event_date = transactions.transaction_date\n",
        "JOIN\n",
        "    interactions ON life_events.cust_id = interactions.cust_id\n",
        "                 AND life_events.life_event_date = interactions.interaction_date;\n",
        "\"\"\", conn)\n",
        "print(query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNLf03V8V7tF",
        "outputId": "63581719-138b-4b2b-82b9-a87ce38c81b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            cust_id      life_event_date     transaction_date  \\\n",
            "0   custid_10009128  2022-08-24 00:00:00  2022-08-24 00:00:00   \n",
            "1   custid_10009376  2021-08-16 00:00:00  2021-08-16 00:00:00   \n",
            "2   custid_10001603  2022-12-03 00:00:00  2022-12-03 00:00:00   \n",
            "3   custid_10005009  2022-04-17 00:00:00  2022-04-17 00:00:00   \n",
            "4   custid_10003036  2022-04-25 00:00:00  2022-04-25 00:00:00   \n",
            "5   custid_10004286  2022-11-29 00:00:00  2022-11-29 00:00:00   \n",
            "6   custid_10004509  2022-05-21 00:00:00  2022-05-21 00:00:00   \n",
            "7   custid_10002330  2021-05-23 00:00:00  2021-05-23 00:00:00   \n",
            "8   custid_10000245  2022-01-24 00:00:00  2022-01-24 00:00:00   \n",
            "9   custid_10004922  2021-05-05 00:00:00  2021-05-05 00:00:00   \n",
            "10  custid_10008338  2022-12-27 00:00:00  2022-12-27 00:00:00   \n",
            "11  custid_10007887  2022-04-10 00:00:00  2022-04-10 00:00:00   \n",
            "12  custid_10007197  2022-12-07 00:00:00  2022-12-07 00:00:00   \n",
            "13  custid_10003799  2021-08-21 00:00:00  2021-08-21 00:00:00   \n",
            "14  custid_10004697  2022-09-19 00:00:00  2022-09-19 00:00:00   \n",
            "15  custid_10008527  2022-06-06 00:00:00  2022-06-06 00:00:00   \n",
            "16  custid_10009903  2021-01-25 00:00:00  2021-01-25 00:00:00   \n",
            "17  custid_10000880  2022-07-21 00:00:00  2022-07-21 00:00:00   \n",
            "18  custid_10004468  2022-05-23 00:00:00  2022-05-23 00:00:00   \n",
            "19  custid_10003212  2021-03-30 00:00:00  2021-03-30 00:00:00   \n",
            "20  custid_10009588  2022-05-04 00:00:00  2022-05-04 00:00:00   \n",
            "21  custid_10002054  2021-09-26 00:00:00  2021-09-26 00:00:00   \n",
            "22  custid_10000829  2022-05-07 00:00:00  2022-05-07 00:00:00   \n",
            "23  custid_10006730  2022-07-14 00:00:00  2022-07-14 00:00:00   \n",
            "24  custid_10006198  2021-04-08 00:00:00  2021-04-08 00:00:00   \n",
            "25  custid_10009080  2022-06-15 00:00:00  2022-06-15 00:00:00   \n",
            "26  custid_10009154  2022-12-20 00:00:00  2022-12-20 00:00:00   \n",
            "27  custid_10008214  2021-08-17 00:00:00  2021-08-17 00:00:00   \n",
            "\n",
            "       interaction_date  \n",
            "0   2022-08-24 00:00:00  \n",
            "1   2021-08-16 00:00:00  \n",
            "2   2022-12-03 00:00:00  \n",
            "3   2022-04-17 00:00:00  \n",
            "4   2022-04-25 00:00:00  \n",
            "5   2022-11-29 00:00:00  \n",
            "6   2022-05-21 00:00:00  \n",
            "7   2021-05-23 00:00:00  \n",
            "8   2022-01-24 00:00:00  \n",
            "9   2021-05-05 00:00:00  \n",
            "10  2022-12-27 00:00:00  \n",
            "11  2022-04-10 00:00:00  \n",
            "12  2022-12-07 00:00:00  \n",
            "13  2021-08-21 00:00:00  \n",
            "14  2022-09-19 00:00:00  \n",
            "15  2022-06-06 00:00:00  \n",
            "16  2021-01-25 00:00:00  \n",
            "17  2022-07-21 00:00:00  \n",
            "18  2022-05-23 00:00:00  \n",
            "19  2021-03-30 00:00:00  \n",
            "20  2022-05-04 00:00:00  \n",
            "21  2021-09-26 00:00:00  \n",
            "22  2022-05-07 00:00:00  \n",
            "23  2022-07-14 00:00:00  \n",
            "24  2021-04-08 00:00:00  \n",
            "25  2022-06-15 00:00:00  \n",
            "26  2022-12-20 00:00:00  \n",
            "27  2021-08-17 00:00:00  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai\n",
        "! pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVlT4iOi7kpj",
        "outputId": "3fdfd0ed-19c8-4ce3-dffd-00a6719ca740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.9-py3-none-any.whl (75 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/75.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.9\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.40.1-py3-none-any.whl (20.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.5)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.101.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.4.0 (from gradio)\n",
            "  Downloading gradio_client-0.4.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.7.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.4.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (2.6.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.9.2)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.3)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=b1abb3aa91d080b3b8622704203331f46c335c14cf06e5364de94f314a5c5503\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, semantic-version, python-multipart, orjson, markdown-it-py, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.0\n",
            "    Uninstalling mdit-py-plugins-0.4.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.101.1 ffmpy-0.3.1 gradio-3.40.1 gradio-client-0.4.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 orjson-3.9.5 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uvicorn-0.23.2 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import gradio\n",
        "\n",
        "openai.api_key="",
        "\"\"\"\n",
        "    Creates the custom GPT-3.5-turbo model for the input/output.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "-GE2ZpeW8Lc9",
        "outputId": "983ac3c9-165c-40da-87d1-c29c9c193a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    Creates the custom GPT-3.5-turbo model for the input/output.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Functions for Model Input/Output\n",
        "\n",
        "Our initial attempts at the solution. In this section, we have documented our initial usage of the GPT-3.5-turbo API to turn text into sql queries. These methods are not utilized in our final solution, but are shown below to document our progress and ideation trial/error.\n",
        "\n",
        "- find_matching_query: returns the matching text query from the training data if the inputted query is identical.\n",
        "- execute_sql_query: queries the existing SQL database from the generated query.\n",
        "- CustomChatGPT: creates an instance of the GPT-3.5-turbo model for our specific queriying purpsose."
      ],
      "metadata": {
        "id": "Fwwm4J8V9_8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "training_data = pd.read_excel('/content/drive/MyDrive/Marketing List Chatbot/Questions/Training.xlsx')\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(training_data['Question'])\n",
        "\n",
        "def find_matching_query(input_question):\n",
        "    \"\"\"\n",
        "    Find the closest matching query from the database for the given input question.\n",
        "    \"\"\"\n",
        "    input_vector = vectorizer.transform([input_question])\n",
        "    cosine_similarities = linear_kernel(input_vector, tfidf_matrix).flatten()\n",
        "\n",
        "\n",
        "    related_docs_indices = cosine_similarities.argsort()[:-2:-1]\n",
        "\n",
        "    if cosine_similarities[related_docs_indices[0]] > 0.5:\n",
        "        return training_data.iloc[related_docs_indices[0]]['Query']\n",
        "    return None\n",
        "\n",
        "def execute_sql_query(query, conn):\n",
        "    \"\"\"\n",
        "    Execute the given SQL query on the provided SQLite connection and return the result.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = pd.read_sql(query, conn)\n",
        "        return str(result.head())\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def CustomChatGPT(input):\n",
        "\n",
        "    local_conn = sqlite3.connect('my_database.db')\n",
        "\n",
        "    matching_query = find_matching_query(input)\n",
        "\n",
        "\n",
        "    if matching_query:\n",
        "        response = execute_sql_query(matching_query, local_conn)\n",
        "        local_conn.close()\n",
        "        return response\n",
        "\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[{'role':'user','content':input}]\n",
        "    )\n",
        "    local_conn.close()\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "interface = gradio.Interface(fn=CustomChatGPT, inputs='text', outputs='text')\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "XE3emkXw77IO",
        "outputId": "729df231-dd9f-4e71-e5fd-a1d375d75a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiboFB3TEpWJ",
        "outputId": "1a9e84f8-a2b1-4b2f-f272-dae6497c33ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(model= \"saikiranmaddukuri/chat_to_sql0.17\")"
      ],
      "metadata": {
        "id": "LXSueyj9EpZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def getQuery(text):\n",
        "  input = text\n",
        "\n",
        "  GPTinput = 'Query: '+ input + 'Dataset : ' + allDataInfo +'For the above mentiond query find the relevant tables and columns from the dataset described above.Follow following rules while doing so: 1. Pick out all tables and columns what could be relevant to the query in format table: CREATE TABLE customers (columns:first_name, last_name, age, marital_status, number_of_dependants, email_address,city, state, zip_postal_code, country, income_level) 2. Above output is input to a text to sql query generator so do not add any lines beside relevant information '\n",
        "\n",
        "  completion = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[{'role':'user','content':GPTinput}]\n",
        "    )\n",
        "  modelInput = input + completion.choices[0].message.content\n",
        "  output = translator([modelInput])\n",
        "  return output"
      ],
      "metadata": {
        "id": "LCpQ8gHcEp0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Solution\n",
        "\n",
        "After much iteration and exploration, our final solution generates accurate text-to-sql queries quickly and concisely. In this section, we present our final method, QueryChatGPT(input), and its interface connection to deploy a functional solution."
      ],
      "metadata": {
        "id": "onYyAqeSqzcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QueryChatGPT(input):\n",
        "\n",
        "    local_conn = sqlite3.connect('my_database.db')\n",
        "\n",
        "\n",
        "    if 'query' in input.split(' '):\n",
        "\n",
        "      response = getQuery(text)\n",
        "      return response\n",
        "\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[{'role':'user','content':input}]\n",
        "    )\n",
        "    local_conn.close()\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "print(\"To query database. Add keyword 'query' before your question \")\n",
        "interface = gradio.Interface(fn=QueryChatGPT, inputs='text', outputs='text')\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "2VYn-ooREVCt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "outputId": "7c310eb9-972a-4c41-9bad-3aa6bf9d41d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7864, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training, Test, and Validation Output Files\n",
        "\n",
        "In this section, we utilize the pandas library to create dataframes that store the training, test, and validation output data, respectively, after utilizing the GPT-3.5-turbo api. Additionally, we use pandas to format the data so that in a [Question, Generated Query] format in each excel file so that it is clean and easy-to-read. Confirmation messages are displayed after each data file has been exported to indicate successful exportation and creation. Finally, we display the time taken to generate the output for each dataset (training, test, validation) through print statements."
      ],
      "metadata": {
        "id": "mKzTZ__rrtgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Data"
      ],
      "metadata": {
        "id": "QXIILVegspkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Marketing List Chatbot/Questions/Training.xlsx'\n",
        "column_name = 'Question'  # Replace with the name of the column you want to extract\n",
        "\n",
        "# Read the Excel file\n",
        "data_frame = pd.read_excel(file_path)\n",
        "#\n",
        "data_frame = data_frame[['Question','Query']]\n",
        "data_frame = data_frame.dropna()\n"
      ],
      "metadata": {
        "id": "dii8yFeRGe6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final =[]\n",
        "for text in data_frame['Question']:\n",
        "  input = text\n",
        "\n",
        "  GPTinput = 'Query: '+ input + 'Dataset : ' + allDataInfo +'For the above mentiond query find the relevant tables and columns from the dataset described above.Follow following rules while doing so: 1. Pick out all tables and columns what could be relevant to the query in format table: CREATE TABLE customers (columns:first_name, last_name, age, marital_status, number_of_dependants, email_address,city, state, zip_postal_code, country, income_level) 2. Above output is input to a text to sql query generator so do not add any lines beside relevant information '\n",
        "\n",
        "  completion = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[{'role':'user','content':GPTinput}]\n",
        "    )\n",
        "  modelInput = input + completion.choices[0].message.content\n",
        "  output = translator([modelInput])\n",
        "  stingHere = [text, output]\n",
        "  final.append(stingHere)\n",
        "\n",
        "df = pd.DataFrame(final, columns=[\"Question\", \"Generated Query\"])\n",
        "# Specify the Excel file name\n",
        "excel_filename = \"content\\output.xlsx\"\n",
        "\n",
        "# Export the DataFrame to Excel\n",
        "df.to_excel(excel_filename, index=False)\n",
        "\n",
        "print(f\"Data exported to {excel_filename}\")"
      ],
      "metadata": {
        "id": "2949MdJSGe-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Time taken for Training Data is 15 min')"
      ],
      "metadata": {
        "id": "iqi_aw5mRKSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Data"
      ],
      "metadata": {
        "id": "KrpruqWOsuTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path1 = '/content/drive/MyDrive/Marketing List Chatbot/Questions/Test.xlsx'\n",
        "column_name = 'Question'  # Replace with the name of the column you want to extract\n",
        "\n",
        "# Read the Excel file\n",
        "data_frame = pd.read_excel(file_path1)\n",
        "#\n",
        "data_frame = data_frame[['Question','Query']]\n",
        "data_frame = data_frame.dropna()\n"
      ],
      "metadata": {
        "id": "xipc7VicKz-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final =[]\n",
        "for text in data_frame['Question']:\n",
        "  input = text\n",
        "\n",
        "  GPTinput = 'Query: '+ input + 'Dataset : ' + allDataInfo +'For the above mentiond query find the relevant tables and columns from the dataset described above.Follow following rules while doing so: 1. Pick out all tables and columns what could be relevant to the query in format table: CREATE TABLE customers (columns:first_name, last_name, age, marital_status, number_of_dependants, email_address,city, state, zip_postal_code, country, income_level) 2. Above output is input to a text to sql query generator so do not add any lines beside relevant information '\n",
        "\n",
        "  completion = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[{'role':'user','content':GPTinput}]\n",
        "    )\n",
        "  modelInput = input + completion.choices[0].message.content\n",
        "  output = translator([modelInput])\n",
        "  stingHere = [text, output]\n",
        "  final.append(stingHere)\n",
        "\n",
        "df = pd.DataFrame(final, columns=[\"Question\", \"Generated Query\"])\n",
        "# Specify the Excel file name\n",
        "excel_filename = \"content\\output_Test.xlsx\"\n",
        "\n",
        "# Export the DataFrame to Excel\n",
        "df.to_excel(excel_filename, index=False)\n",
        "\n",
        "print(f\"Data exported to {excel_filename}\")"
      ],
      "metadata": {
        "id": "Y6ZLo0vlQaf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Time taken is 6 Min')"
      ],
      "metadata": {
        "id": "fXnWv3JfSesL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation Data"
      ],
      "metadata": {
        "id": "vMTkUFhVsxUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path2 = '/content/drive/MyDrive/Marketing List Chatbot/Questions/Validation.xlsx'\n",
        "column_name = 'Question'  # Replace with the name of the column you want to extract\n",
        "\n",
        "# Read the Excel file\n",
        "data_frame = pd.read_excel(file_path1)\n",
        "#\n",
        "data_frame = data_frame[['Question','Query']]\n",
        "data_frame = data_frame.dropna()\n"
      ],
      "metadata": {
        "id": "je6FSRwpSizn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final =[]\n",
        "for text in data_frame['Question']:\n",
        "  input = text\n",
        "\n",
        "  GPTinput = 'Query: '+ input + 'Dataset : ' + allDataInfo +'For the above mentiond query find the relevant tables and columns from the dataset described above.Follow following rules while doing so: 1. Pick out all tables and columns what could be relevant to the query in format table: CREATE TABLE customers (columns:first_name, last_name, age, marital_status, number_of_dependants, email_address,city, state, zip_postal_code, country, income_level) 2. Above output is input to a text to sql query generator so do not add any lines beside relevant information '\n",
        "\n",
        "  completion = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[{'role':'user','content':GPTinput}]\n",
        "    )\n",
        "  modelInput = input + completion.choices[0].message.content\n",
        "  output = translator([modelInput])\n",
        "  stingHere = [text, output]\n",
        "  final.append(stingHere)\n",
        "\n",
        "df = pd.DataFrame(final, columns=[\"Question\", \"Generated Query\"])\n",
        "# Specify the Excel file name\n",
        "excel_filename = \"content\\output_Validation.xlsx\"\n",
        "\n",
        "# Export the DataFrame to Excel\n",
        "df.to_excel(excel_filename, index=False)\n",
        "\n",
        "print(f\"Data exported to {excel_filename}\")"
      ],
      "metadata": {
        "id": "5meahUIjS1fi",
        "outputId": "90d25ff6-ede0-418b-b963-efd88fecc672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data exported to content\\output_Validation.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Accuracy of the Model: Levenshtein Distance\n",
        "\n",
        "For the accuracy evaluation, we decided to utilize the Levenshtein Distance calculation in order to determine how closely our model was generating queries compared to the expected output. We utilized the Levenshtein Distance similarity score as a quantitative measure to determine the similarity of the expected output and model output."
      ],
      "metadata": {
        "id": "HCvhEeGus2DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein"
      ],
      "metadata": {
        "id": "WoGUNHOcYijM",
        "outputId": "8dbfab00-efaf-4d20-9dc6-ddde691cc79d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.21.1 rapidfuzz-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import Levenshtein\n",
        "\n",
        "def compare_sql_similarity(sql1, sql2):\n",
        "    # Preprocessing\n",
        "    sql1 = sql1.lower().strip()\n",
        "    sql2 = sql2.lower().strip()\n",
        "\n",
        "    # Tokenization (assuming tokens are separated by spaces)\n",
        "    tokens1 = sql1.split()\n",
        "    tokens2 = sql2.split()\n",
        "\n",
        "    # Normalization (replace table and column names with placeholders)\n",
        "    normalized_tokens1 = [token for token in tokens1]\n",
        "    normalized_tokens2 = [token for token in tokens2]\n",
        "\n",
        "    # Calculate Levenshtein distance\n",
        "    levenshtein_distance = Levenshtein.distance(\" \".join(normalized_tokens1), \" \".join(normalized_tokens2))\n",
        "\n",
        "    return levenshtein_distance\n",
        "\n",
        "sql1 = \"SELECT c.customer_id, c.first_name, c.last_name FROM customers c WHERE c.marital_status = 'married' AND c.number_of_dependents >= 2 AND c.state = 'Arizona';\"\n",
        "sql2 = \"SELECT marital_status, number_of_dependants, state FROM customers WHERE state = AZ AND first_name = 'Marriage' AND number_of_dependants >= 2\"\n",
        "\n",
        "\n",
        "def levenshtein_similarity_score(str1, str2):\n",
        "    # Calculate Levenshtein distance\n",
        "    distance = compare_sql_similarity(str1,str2)\n",
        "\n",
        "    # Calculate maximum possible distance (longer of the two strings)\n",
        "    max_possible_distance = max(len(str1), len(str2))\n",
        "\n",
        "    # Calculate similarity score as (1 - (distance / max_possible_distance)) * 100\n",
        "    similarity_score = (1 - distance / max_possible_distance) * 100\n",
        "\n",
        "    return similarity_score\n"
      ],
      "metadata": {
        "id": "7MkR-LNiYwQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results Analysis\n",
        "\n",
        "Below, we implement the Levenshtein Distance on the training and test data where we observe similar results in the accuracy scores."
      ],
      "metadata": {
        "id": "-WtZ4oJpwXjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the function that takes two query lists as arguments\n",
        "def process_queries(query_list1, query_list2):\n",
        "    score=0\n",
        "    for query1, query2 in zip(query_list1, query_list2):\n",
        "        # Your processing logic here\n",
        "        score = score + levenshtein_similarity_score(query1, query2)\n",
        "    return score\n",
        "\n",
        "# Load the data from Excel files into DataFrames\n",
        "file_path = '/content/drive/MyDrive/Marketing List Chatbot/Questions/Training.xlsx'\n",
        "# Read the Excel file\n",
        "data_frame = pd.read_excel(file_path)\n",
        "data_frame = data_frame[['Question','Query']]\n",
        "data_frame = data_frame.dropna()\n",
        "\n",
        "file1_df = pd.read_excel(\"/content/contentoutput-2.xlsx\")\n",
        "file2_df = data_frame\n",
        "\n",
        "# Extract the \"Query\" column as lists\n",
        "query_list1 = file1_df[\"Generated Query\"].tolist()\n",
        "query_list2 = file2_df[\"Query\"].tolist()\n",
        "\n",
        "# Call the function with the extracted query lists\n",
        "score = process_queries(query_list1, query_list2)\n",
        "\n",
        "print(\" Accuracy on Training Data  - \" , score/len(query_list1) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOoOMX48bR4f",
        "outputId": "098d57cd-c685-4cd6-df59-8e9a651d25ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy on Training Data  -  28.39148538947133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function that takes two query lists as arguments\n",
        "def process_queries(query_list1, query_list2):\n",
        "    score=0\n",
        "    for query1, query2 in zip(query_list1, query_list2):\n",
        "        score = score + levenshtein_similarity_score(query1, query2)\n",
        "    return score\n",
        "\n",
        "# Load the data from Excel files into DataFrames\n",
        "file_path = '/content/drive/MyDrive/Marketing List Chatbot/Questions/Test.xlsx'\n",
        "# Read the Excel file\n",
        "data_frame = pd.read_excel(file_path)\n",
        "data_frame = data_frame[['Question','Query']]\n",
        "data_frame = data_frame.dropna()\n",
        "\n",
        "file1_df = pd.read_excel(\"/content/contentoutput_Test.xlsx\")\n",
        "file2_df = data_frame\n",
        "\n",
        "# Extract the \"Query\" column as lists\n",
        "query_list1 = file1_df[\"Generated Query\"].tolist()\n",
        "query_list2 = file2_df[\"Query\"].tolist()\n",
        "\n",
        "# Call the function with the extracted query lists\n",
        "score = process_queries(query_list1, query_list2)\n",
        "\n",
        "print(\" Accuracy on TestData  - \" , score/len(query_list1) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU4yFcuabR8M",
        "outputId": "51396831-e0a4-4ee7-8e96-f138a0e36be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy on TestData  -  28.509610039209267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future Avenues of Development\n",
        "\n",
        "Our model just scratches the surface of larger NLP projects. To improve upon our project we could implement the following features/ideas.\n",
        "\n",
        "1. To improve the accuracy of the data (and generate a smaller Levenshtein distance where the strings are more similar), we could train the data on a larger dataset and refine our inital propmts to the gpt-3.5-turbo api.\n",
        "2. We could improve the User Interface of our project to provide suggested queries or text to reduce typing time and allow users to quickly recieve results. We could conduct a similar analysis on the most popular queries and expand the UI to add these suggestions.\n",
        "3. Utilize additional measures to evaluate accuracy such as Jaro-Winkler to get a more comprehensive approach of how our model is performing."
      ],
      "metadata": {
        "id": "pBqgf9rGuJJt"
      }
    }
  ]
}
